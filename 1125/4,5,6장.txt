4장
Linear Regression 연속적 변수 예측
- Simple Linear regression (y = w0 + w1 x, y : dependent variables, x : independent variables), 손실함수 : 에러(예측값과 실제값의 차) 제곱의 합, 미분을 통해 최솟값을 나타내는 w0, w1을 구함 (낮을수록 성능이 좋음)
- Multiple regression 다양한 변수를 input
- Gradient Descent 경사 하강법 : 초기값을 정하고 점점 업데이트하면서 최소화되는 형태로 움직임.


5장
Logistic Regression 분류 모델을 예측
- P = w0 + w1x -> 좌변과 우변의 범위가 맞지 않음 -> ln(p/(1-p)) = w0 + w1x -> p = 1 /(1+e^(-z)) (z = w0 + w1x …)(= sigmoid function)
- MLE(maximum likelihood estimation) 모델이 주어졌을때 data 대로 나올 확률 (높을수록 성능이 좋음) -> 손실함수 : -log(P(likelihood)) -> … -> -(y-y*)xi
- GD : 전체 데이터로 업데이트
- SGD : 개별 데이터마다 업데이트
- Mini-batch SGD : 데이터를 묶어서 업데이트


6장
Performance of Learning
- K-fold cross-validation : 데이터를 나누어서 학습하는 데이터를 계속 다르게 설정해서 훈련시키고 나머지 데이터로 확인
Overfitting 과적합
- 기존 데이터에 너무 취중되어 있어 새로운 데이터의 정확도가 떨어짐 (너무 모델이 복잡하거나, 정보가 적을 때)
- Bias : 평균적으로 전반적인 에러가 있음
- Variance : 일관성이 없음
- 모델을 간소화 : Preprinting (정보의 이득을 기준으로 결정), Postponing (의미가 없다고 여겨지는 가지를 삭제)
- 데이터를 증가
Instance-Based Learning : 데이터를 기반으로 예측
- K-nearest neighbor method : 유사한 K개 데이터로 예측
- Normalization : feature value 마다 차이의 단위가 다름
- High dimensional data : ??


